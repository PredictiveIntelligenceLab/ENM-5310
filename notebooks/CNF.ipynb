{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import jax\n",
    "from typing import Any, Callable, Sequence, Optional, NewType\n",
    "from jax import lax, random, vmap, scipy, numpy as jnp\n",
    "\n",
    "from ode import odeint\n",
    "import flax\n",
    "from flax.training import train_state\n",
    "from flax import traverse_util\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "from flax import serialization\n",
    "import optax\n",
    "from sklearn.datasets import make_circles, make_moons, make_s_curve\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperNetwork(nn.Module):\n",
    "    \"\"\"Hyper-network allowing f(z(t), t) to change with time.\n",
    "    Adapted from the Pytorch implementation at:\n",
    "    https://github.com/rtqichen/torchdiffeq/blob/master/examples/cnf.py\n",
    "    \"\"\"\n",
    "    in_out_dim: Any = 2\n",
    "    hidden_dim: Any = 32\n",
    "    width: Any = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, t):\n",
    "        # predict params\n",
    "        blocksize = self.width * self.in_out_dim\n",
    "        params = lax.expand_dims(t, (0, 1))\n",
    "        params = nn.Dense(self.hidden_dim)(params)\n",
    "        params = nn.tanh(params)\n",
    "        params = nn.Dense(self.hidden_dim)(params)\n",
    "        params = nn.tanh(params)\n",
    "        params = nn.Dense(3 * blocksize + self.width)(params)\n",
    "\n",
    "        # restructure\n",
    "        params = lax.reshape(params, (3 * blocksize + self.width,))\n",
    "        W = lax.reshape(params[:blocksize], (self.width, self.in_out_dim, 1))\n",
    "\n",
    "        U = lax.reshape(params[blocksize:2 * blocksize], (self.width, 1, self.in_out_dim))\n",
    "\n",
    "        G = lax.reshape(params[2 * blocksize:3 * blocksize], (self.width, 1, self.in_out_dim))\n",
    "        U = U * nn.sigmoid(G)\n",
    "\n",
    "        B = lax.expand_dims(params[3 * blocksize:], (1, 2))\n",
    "        return W, B, U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNF(nn.Module):\n",
    "    \"\"\"Adapted from the Pytorch implementation at:\n",
    "    https://github.com/rtqichen/torchdiffeq/blob/master/examples/cnf.py\n",
    "    \"\"\"\n",
    "    in_out_dim: Any = 2\n",
    "    hidden_dim: Any = 32\n",
    "    width: Any = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, t, states):\n",
    "        z, logp_z = states[:, :2], states[:, 2:]\n",
    "        W, B, U = HyperNetwork(self.in_out_dim, self.hidden_dim, self.width)(t)\n",
    "\n",
    "        def dzdt(z):\n",
    "            h = nn.tanh(vmap(jnp.matmul, (None, 0))(z, W) + B)\n",
    "            return jnp.matmul(h, U).mean(0)\n",
    "\n",
    "        dz_dt = dzdt(z)\n",
    "        sum_dzdt = lambda z: dzdt(z).sum(0)\n",
    "        df_dz = jax.jacrev(sum_dzdt)(z)\n",
    "        dlogp_z_dt = -1.0 * jnp.trace(df_dz, 0, 0, 2)\n",
    "\n",
    "        return lax.concatenate((dz_dt, lax.expand_dims(dlogp_z_dt, (1,))), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neg_CNF(nn.Module):\n",
    "    \"\"\"Negative CNF for jax's odeint.\"\"\"\n",
    "    in_out_dim: Any = 2\n",
    "    hidden_dim: Any = 32\n",
    "    width: Any = 64\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, t, states):\n",
    "        outputs = CNF(self.in_out_dim, self.hidden_dim, self.width)(-1.0 * t, states)\n",
    "\n",
    "        return -1.0 * outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_circles(num_samples):\n",
    "    \"\"\"Adapted from the Pytorch implementation at:\n",
    "    https://github.com/rtqichen/torchdiffeq/blob/master/examples/cnf.py\n",
    "    \"\"\"\n",
    "    points, _ = make_circles(n_samples=num_samples, noise=0.06, factor=0.5)\n",
    "    x = jnp.array(points, dtype=jnp.float32)\n",
    "    logp_diff_t1 = jnp.zeros((num_samples, 1), dtype=jnp.float32)\n",
    "\n",
    "    return lax.concatenate((x, logp_diff_t1), 1)\n",
    "\n",
    "\n",
    "def get_batch_moons(num_samples):\n",
    "    points, _ = make_moons(n_samples=num_samples, noise=0.05)\n",
    "    x = jnp.array(points, dtype=jnp.float32)\n",
    "    logp_diff_t1 = jnp.zeros((num_samples, 1), dtype=jnp.float32)\n",
    "\n",
    "    return lax.concatenate((x, logp_diff_t1), 1)\n",
    "\n",
    "\n",
    "def get_batch_scurve(num_samples):\n",
    "    points, _ = make_s_curve(n_samples=num_samples, noise=0.05, random_state=0)\n",
    "    x1 = jnp.array(points, dtype=jnp.float32)[:, :1]\n",
    "    x2 = jnp.array(points, dtype=jnp.float32)[:, 2:]\n",
    "    x = lax.concatenate((x1, x2), 1)\n",
    "    logp_diff_t1 = jnp.zeros((num_samples, 1), dtype=jnp.float32)\n",
    "\n",
    "    return lax.concatenate((x, logp_diff_t1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_normal(z):\n",
    "    \"\"\"\n",
    "    Log probability of multivariate_normal.\n",
    "    \"\"\"\n",
    "    mean = jnp.array([0., 0.])\n",
    "    z_m = z - mean\n",
    "    cov = jnp.array([[0.1, 0.], [0., 0.1]])\n",
    "    logz = -jnp.log((2 * jnp.pi)) + -0.5 * jnp.log(jnp.linalg.det(cov)) + -0.5 * jnp.matmul(jnp.matmul(z_m.T, jnp.linalg.inv(cov)), z_m)\n",
    "    return logz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(params):\n",
    "    # Convert all value of Params to certain constant\n",
    "    params = unfreeze(params)\n",
    "    # Get flattened-key: value list.\n",
    "    flat_params = {'/'.join(k): v for k, v in traverse_util.flatten_dict(params).items()}\n",
    "    unflat_params = traverse_util.unflatten_dict({tuple(k.split('/')): 0.1 * jnp.ones_like(v) for k, v in flat_params.items()})\n",
    "    new_params = freeze(unflat_params)\n",
    "    test_x = jnp.array([[0., 1.], [2., 3.], [4., 5.]])\n",
    "    test_log_p = jnp.zeros((3, 1))\n",
    "    test_inputs = lax.concatenate((test_x, test_log_p), 1)\n",
    "    Neg_CNF().apply({'params': new_params}, jnp.array(0.), test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, learning_rate, in_out_dim, hidden_dim, width):\n",
    "    \"\"\"Creates initial 'TrainState'.\"\"\"\n",
    "    inputs = jnp.ones((1, 2))\n",
    "    neg_cnf = Neg_CNF(in_out_dim, hidden_dim, width)\n",
    "    params = neg_cnf.init(rng, jnp.array(10.), inputs)['params']\n",
    "    set_params(params)\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=neg_cnf.apply, params=params, tx=tx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(2, 3, 4, 5, 6))\n",
    "def train_step(state, batch, in_out_dim, hidden_dim, width, t0, t1):\n",
    "    p_z0 = lambda x: scipy.stats.multivariate_normal.logpdf(x,\n",
    "                                                            mean=jnp.array([0., 0.]),\n",
    "                                                            cov=jnp.array([[0.1, 0.], [0., 0.1]]))\n",
    "    vmap_multi = jax.vmap(multivariate_normal, 0, 0)\n",
    "    def loss_fn(params):\n",
    "        func = lambda states, t: Neg_CNF(in_out_dim, hidden_dim, width).apply({'params': params}, t, states)\n",
    "        outputs = odeint(\n",
    "            func,\n",
    "            batch,\n",
    "            -1.0 * jnp.array([t1, t0]),\n",
    "            atol=1e-5,\n",
    "            rtol=1e-5\n",
    "        )\n",
    "        z_t, logp_diff_t = outputs[:, :, :2], outputs[:, :, 2:]\n",
    "        z_t0, logp_diff_t0 = z_t[-1], logp_diff_t[-1]\n",
    "        logp_x = p_z0(z_t0) - lax.squeeze(logp_diff_t0, dimensions=(1,))\n",
    "        loss = -logp_x.mean(0)\n",
    "        return loss\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_dynamics(dynamics_fn, initial_state, t):\n",
    "    def f(initial_state, t):\n",
    "        return odeint(dynamics_fn, initial_state, t, atol=1e-5, rtol=1e-5)\n",
    "    return f(initial_state, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(neg_params, pos_params, in_out_dim, hidden_dim, width, t0, t1, dataset):\n",
    "    \"\"\"Adapted from PyTorch \"\"\"\n",
    "    viz_samples = 30000\n",
    "    viz_timesteps = 41\n",
    "    if dataset == \"circles\":\n",
    "        get_batch = lambda num_samples: get_batch_circles(num_samples)\n",
    "    elif dataset == \"moons\":\n",
    "        get_batch = lambda num_samples: get_batch_moons(num_samples)\n",
    "    elif dataset == \"scurve\":\n",
    "        get_batch = lambda num_samples: get_batch_scurve(num_samples)\n",
    "    target_sample = get_batch(viz_samples)[:, :2]\n",
    "\n",
    "    if not os.path.exists('results_%s/' % dataset):\n",
    "        os.makedirs('results_%s/' % dataset)\n",
    "\n",
    "    z_t0 = jnp.array(np.random.multivariate_normal(mean=np.array([0., 0.]),\n",
    "                                                   cov=np.array([[0.1, 0.], [0., 0.1]]),\n",
    "                                                   size=viz_samples))\n",
    "    logp_diff_t0 = jnp.zeros((viz_samples, 1), dtype=jnp.float32)\n",
    "\n",
    "    func_pos = lambda states, t: CNF(in_out_dim, hidden_dim, width).apply({'params': pos_params}, t, states)\n",
    "    output = solve_dynamics(func_pos, lax.concatenate((z_t0, logp_diff_t0), 1), jnp.linspace(t0, t1, viz_timesteps))\n",
    "    z_t_samples, _ = output[..., :2], output[..., 2:]\n",
    "\n",
    "    # Generate evolution of density\n",
    "    x = jnp.linspace(-1.5, 1.5, 100)\n",
    "    y = jnp.linspace(-1.5, 1.5, 100)\n",
    "    points = np.vstack(np.meshgrid(x, y)).reshape([2, -1]).T\n",
    "\n",
    "    z_t1 = jnp.array(points, dtype=jnp.float32)\n",
    "    logp_diff_t1 = jnp.zeros((z_t1.shape[0], 1), dtype=jnp.float32)\n",
    "    func_neg = lambda states, t: Neg_CNF(in_out_dim, hidden_dim, width).apply({'params': neg_params}, t, states)\n",
    "    output = solve_dynamics(func_neg, lax.concatenate((z_t1, logp_diff_t1), 1), -jnp.linspace(t1, t0, viz_timesteps))\n",
    "    z_t_density, logp_diff_t = output[..., :2], output[..., 2:]\n",
    "\n",
    "    return z_t_samples, z_t_density, logp_diff_t, viz_timesteps, target_sample, z_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(z_t_samples, z_t_density, logp_diff_t, t0, t1, viz_timesteps, target_sample, z_t1, dataset):\n",
    "    # Create plots for each timestep\n",
    "    for (t, z_sample, z_density, logp_diff) in zip(\n",
    "            tqdm(np.linspace(t0, t1, viz_timesteps)),\n",
    "            z_t_samples, z_t_density, logp_diff_t\n",
    "    ):\n",
    "        fig = plt.figure(figsize=(12, 4), dpi=200)\n",
    "        plt.tight_layout()\n",
    "        plt.axis('off')\n",
    "        plt.margins(0, 0)\n",
    "        fig.suptitle(f'{t:.2f}s')\n",
    "\n",
    "        ax1 = fig.add_subplot(1, 3, 1)\n",
    "        ax1.set_title('Target')\n",
    "        ax1.get_xaxis().set_ticks([])\n",
    "        ax1.get_yaxis().set_ticks([])\n",
    "        ax2 = fig.add_subplot(1, 3, 2)\n",
    "        ax2.set_title('Samples')\n",
    "        ax2.get_xaxis().set_ticks([])\n",
    "        ax2.get_yaxis().set_ticks([])\n",
    "        ax3 = fig.add_subplot(1, 3, 3)\n",
    "        ax3.set_title('Log Probability')\n",
    "        ax3.get_xaxis().set_ticks([])\n",
    "        ax3.get_yaxis().set_ticks([])\n",
    "\n",
    "        ax1.hist2d(*jnp.transpose(target_sample), bins=300, density=True,\n",
    "                   range=[[-1.5, 1.5], [-1.5, 1.5]])\n",
    "\n",
    "        ax2.hist2d(*jnp.transpose(z_sample), bins=300, density=True,\n",
    "                   range=[[-1.5, 1.5], [-1.5, 1.5]])\n",
    "        p_z0 = lambda x: scipy.stats.multivariate_normal.logpdf(x,\n",
    "                                                                mean=jnp.array([0., 0.]),\n",
    "                                                                cov=jnp.array([[0.1, 0.], [0., 0.1]]))\n",
    "        logp = p_z0(z_density) - lax.squeeze(logp_diff, dimensions=(1,))\n",
    "        ax3.tricontourf(*jnp.transpose(z_t1),\n",
    "                        jnp.exp(logp), 200)\n",
    "\n",
    "        plt.savefig(os.path.join('results_%s/' % dataset, f\"cnf-viz-{int(t * 1000):05d}.jpg\"),\n",
    "                    pad_inches=0.2, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    img, *imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join('results_%s/' % dataset, f\"cnf-viz-*.jpg\")))]\n",
    "    img.save(fp=os.path.join('results_%s/' % dataset, \"cnf-viz.gif\"), format='GIF', append_images=imgs,\n",
    "             save_all=True, duration=250, loop=0)\n",
    "\n",
    "    print('Saved visualization animation at {}'.format(os.path.join('results_%s/' % dataset, \"cnf-viz.gif\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:19<00:00, 75.37it/s, loss=0.34721833]\n",
      "100%|██████████| 41/41 [00:17<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualization animation at results_moons/cnf-viz.gif\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train the model.\"\"\"\n",
    "learning_rate = 1e-3\n",
    "n_iters = 1500\n",
    "batch_size = 512\n",
    "in_out_dim = 2\n",
    "hidden_dim = 32\n",
    "width = 64\n",
    "t0 = 0.0\n",
    "t1 = 10.0\n",
    "visual = True \n",
    "dataset = 'moons'\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, learning_rate, in_out_dim, hidden_dim, width)\n",
    "if dataset == \"circles\":\n",
    "    get_batch = lambda num_samples: get_batch_circles(num_samples)\n",
    "elif dataset == \"moons\":\n",
    "    get_batch = lambda num_samples: get_batch_moons(num_samples)\n",
    "elif dataset == \"scurve\":\n",
    "    get_batch = lambda num_samples: get_batch_scurve(num_samples)\n",
    "\n",
    "pbar = trange(1, n_iters+1)\n",
    "for itr in pbar:\n",
    "    batch = get_batch(batch_size)\n",
    "    state, loss = train_step(state, batch, in_out_dim, hidden_dim, width, t0, t1)\n",
    "    pbar.set_postfix({'loss': loss})\n",
    "\n",
    "if visual is True:\n",
    "    # Convert Params of Neg_CNF to CNF\n",
    "    neg_params = state.params\n",
    "    neg_params = unfreeze(neg_params)\n",
    "    # Get flattened-key: value list.\n",
    "    neg_flat_params = {'/'.join(k): v for k, v in traverse_util.flatten_dict(neg_params).items()}\n",
    "    pos_flat_params = {key[6:]: jnp.array(np.array(neg_flat_params[key])) for key in list(neg_flat_params.keys())}\n",
    "    pos_unflat_params = traverse_util.unflatten_dict({tuple(k.split('/')): v for k, v in pos_flat_params.items()})\n",
    "    pos_params = freeze(pos_unflat_params)\n",
    "    # jax.profiler.save_device_memory_profile(\"memory.prof\")\n",
    "    output = viz(neg_params, pos_params, in_out_dim, hidden_dim, width, t0, t1, dataset)\n",
    "    z_t_samples, z_t_density, logp_diff_t, viz_timesteps, target_sample, z_t1 = output\n",
    "    create_plots(z_t_samples, z_t_density, logp_diff_t, t0, t1, viz_timesteps, target_sample, z_t1, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
